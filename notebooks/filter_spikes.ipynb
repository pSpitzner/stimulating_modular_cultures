{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering spiking data\n",
    "\n",
    "In this notebook we simulate the effect of applying MLSpike to the traces generated by our spike trains. In order to do so, we follow Huang et al., eLife 2021. Here, the autors report the probabilities of detecting an action potential, given the amount of events in a bin of 250ms. \n",
    "\n",
    "In order to use this notebook, one first needs to generate a simulated HDF5 file. For more instructions on how to do so, please refer to the `spiking_neuron_model` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autoreload extension allows you to tweak the code in the imported modules\n",
    "# and rerun cells to reflect the changes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# this allows us to output the dictionary structure\n",
    "#%load_ext ipy_dict_hierarchy\n",
    "# matplotlib settings to look decent in the notebook\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import sys\n",
    "# the helpers are in the `/ana/` folder.\n",
    "sys.path.append(\"./../\")\n",
    "from ana import ana_helper as ah\n",
    "from ana import plot_helper as ph\n",
    "from ana import paper_plots as ppl\n",
    "\n",
    "#Libraries to generate results from the notebook itself.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from benedict import benedict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values below are the measured probabilities obtained from Figure 7 of Huang et al. These where obtained just by analysing the figure with dedicated software. The cases correspond to the MLSpike bars, and different experimental conditions. Which one is used for filtering is indicated later as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Values of probabilities from the ALife paper have been scrapped and they are hardcoded here\n",
    "#for the different conditions\n",
    "prob_values = {}\n",
    "prob_values[\"hr_emxs\"] = np.array([0.0958, 0.373, 0.575, 0.695, 0.811])\n",
    "prob_values[\"hr_tetos\"] = np.array([0.194, 0.553, 0.821, 0.7913, 1.0])\n",
    "prob_values[\"hr_cuxf\"] = np.array([0.096, 0.280, 0.533, 0.568, 0.698])\n",
    "prob_values[\"hr_emxf\"] = np.array([0.054, 0.396, 0.580, 0.752, 0.916])\n",
    "\n",
    "prob_values[\"lr_emxs\"] = np.array([0.052, 0.236, 0.423, 0.592, 0.700])\n",
    "prob_values[\"lr_tetos\"] = np.array([0.106, 0.469, 0.722, 0.939, 1.0])\n",
    "prob_values[\"lr_cuxf\"] = np.array([0.081, 0.329, 0.649, 0.759, 0.897])\n",
    "prob_values[\"lr_emxf\"] = np.array([0.049, 0.349, 0.521, 0.624, 0.690])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions\n",
    "\n",
    "These functions are in charge of binning raster plot and performing the filtering from the given spike train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_spikes(spiketimes, bintime=0.25, tf=1200):\n",
    "    \"\"\"\n",
    "    Given an array of spike times, produces a binned timeseries, as a well as an nd-array which contains\n",
    "    the index of the bin to which each spike time belongs to.\n",
    "    Parameters\n",
    "    ==========\n",
    "    - spiketimes: ndarray\n",
    "        A NxK array which has the spike times (in seconds) for the all neurons. \n",
    "        N is number of neurons, and K the maximum number of spikes. \n",
    "        Non-used spaces of the array are coded with NaNs.\n",
    "    - bintime: float\n",
    "        Time width of the bin to be used, in seconds. Default is 250ms.\n",
    "    - tf: float\n",
    "        Total simulation time, in seconds\n",
    "    Returns\n",
    "    =======\n",
    "    - binned: numpy array\n",
    "        1-d array that contains the amount of spikes in each bin, as integers.\n",
    "    - which_bin: ndarray\n",
    "        An array with shape NxK which instead of having spiketimes contains the index\n",
    "        of the correspondent bins. \n",
    "    \"\"\"\n",
    "    \n",
    "    #Gerate the bins\n",
    "    bins = np.arange(0, tf, bintime)\n",
    "    \n",
    "    #Here we don't care which neuron generated the spike. Just delete nans.\n",
    "    data = spiketimes.flatten() \n",
    "    data = data[~np.isnan(data)]\n",
    "    \n",
    "    #Histogram produces the binned activity timeseries. Digitize is a handy function to get in\n",
    "    #which bin everything was\n",
    "    return np.histogram(data, bins=bins, density=False)[0], np.digitize(spiketimes, bins=bins)\n",
    "\n",
    "\n",
    "def prob_2_use(spikecount, prob_detection):\n",
    "    \"\"\"\n",
    "    Selects which probability will be used in each bin, depending on the number of action potential\n",
    "    in such bins.\n",
    "    Parameters\n",
    "    ==========\n",
    "    - spikecount: ndarray\n",
    "        A NxK array which has the spike times (in seconds) for the all neurons. \n",
    "        See \"bin_spikes\" docs for more info.\n",
    "    - prob_detection: ndarray\n",
    "        1-dim array with 5 elements that contains the probability P(k) of detecting 1 AP if there \n",
    "        is k spikes in the bins\n",
    "    Returns\n",
    "    =======\n",
    "    - probs: ndarray\n",
    "        An array which contains the probability of detection for each time bin.\n",
    "    \"\"\"\n",
    "    \n",
    "    #By default, probability of detection is 1\n",
    "    values = np.ones(spikecount.size)\n",
    "    \n",
    "    #If there was nothing, nothing to do; \n",
    "    #If there is 6+ spikes, assume prob=100%\n",
    "    mask = (spikecount <= 5) & (spikecount > 0)\n",
    "    \n",
    "    #Apply mask\n",
    "    values[mask] = prob_detection[spikecount[mask] - 1] \n",
    "    return values\n",
    "\n",
    "def spikes_not_detected(binned, prob_values, prob_table=\"hr_emxs\"):\n",
    "    \"\"\"\n",
    "    Returns a new array with the spike count after the filtering is applied.\n",
    "    Parameters\n",
    "    ==========\n",
    "    - binned: ndarray\n",
    "        Contains the total number of spikes in each bin\n",
    "    - prob_values: dict\n",
    "        This dictionary contains the probability P(k) of detecting 1 spike if there \n",
    "        is k spikes in the bins for different experimental cases. Which case is selected\n",
    "        depends on \"prob_table\" argument. Default is \"hr_emxs\".\n",
    "    - p_table: string\n",
    "        To select one entry from the prob_values dictionary. Possible values are shown in the\n",
    "        filter_spikes notebook\n",
    "    Returns\n",
    "    =======\n",
    "    - detected_spikes: ndarray\n",
    "        Contains the detected number of spikes in each bin after the filter is applied\n",
    "    \"\"\"  \n",
    "    #Get the probabilities to use in each bin\n",
    "    probs = prob_2_use(binned, prob_values[prob_table])\n",
    "    #Generate values according to a binomial distribution with \"n=binned, p=probs\" for each bin.\n",
    "    return np.random.binomial(binned, probs) \n",
    "\n",
    "\n",
    "def delete_from_spikeseries(binned, masked, which_bin, spiketimes):\n",
    "    \"\"\"\n",
    "    Given the actual amount of spikes and the result after filtering, erase randomly \n",
    "    some action potentials from the spiketimes, without modyfying it.\n",
    "    Parameters\n",
    "    ==========\n",
    "    - binned: ndarray\n",
    "        Contains the total number of spikes in each bin\n",
    "    - masked: ndarray\n",
    "        Contains the detected number of spikes in each bin after the filter is applied\n",
    "    - which_bin: ndarray\n",
    "        An array with shape NxK which instead of having spiketimes contains the index\n",
    "        of the correspondent bins.\n",
    "    - spiketimes: ndarray\n",
    "        A NxK array which has the spike times (in seconds) for the all neurons. \n",
    "        See \"bin_spikes\" docs for more info.\n",
    "    Returns\n",
    "    =======\n",
    "    - new_spiketimes: ndarray\n",
    "        Same as spiketimes, but with some randomly deleted spikes to match the masked array. \n",
    "        These are marked with a \"-1\" value\n",
    "    \"\"\"  \n",
    "    \n",
    "    #Initialize the arrays\n",
    "    corrected = np.zeros(binned.size)\n",
    "    n_neurons, n_times = spiketimes.shape\n",
    "    new_spikes = spiketimes.copy()\n",
    "    \n",
    "    #Loop on times and after that in neurons, to delete spikes \"vertically\" in the raster\n",
    "    for j in range(n_times):\n",
    "        for i in range(n_neurons):\n",
    "            #In which bin is this spiketime?\n",
    "            index = which_bin[i,j] - 1 \n",
    "            \n",
    "            #Nans are coded as 4800, so everything that is 4799 is not valid\n",
    "            if index < binned.size-1:\n",
    "                #binned- masked = number of deleted spikes. So keep deleting\n",
    "                #until these two are equal\n",
    "                if masked[index] + corrected[index] < binned[index]:\n",
    "                    corrected[index] += 1\n",
    "                    new_spikes[i, j] = -1\n",
    "    return new_spikes\n",
    "\n",
    "\n",
    "def prepare_file(path):\n",
    "    # quick helper to do preprocessing matching the usual settings\n",
    "    bs_large = 20 / 1000 # ms\n",
    "    threshold_factor = 2.5 / 100 # %\n",
    "    remove_null_sequences = False\n",
    "\n",
    "    h5f = ah.prepare_file(path, hot=False)\n",
    "    ah.find_rates(h5f, bs_large=bs_large)\n",
    "    ah.find_system_bursts_from_global_rate(\n",
    "        h5f,\n",
    "        rate_threshold=threshold_factor * np.nanmax(h5f[\"ana.rates.system_level\"]),\n",
    "        merge_threshold=0.1,\n",
    "        skip_sequences=False,\n",
    "    )\n",
    "\n",
    "    # this is a global setting for now\n",
    "    if remove_null_sequences:\n",
    "        ah.remove_bursts_with_sequence_length_null(h5f)\n",
    "\n",
    "    ah.find_ibis(h5f)\n",
    "    ah.find_participating_fraction_in_bursts(h5f)\n",
    "    ah.find_isis(h5f)\n",
    "\n",
    "    return h5f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "Use the functions defined above to process the data. After that, the spiketrain inside the variable `h5f_new` is substituted by the filtered one, and graphs are produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of filtering on burst observables\n",
    "\n",
    "Now we do check the effect of the filtering in some of the system observables. To do so, we first run the function to find rates, which are subsequently used to find bursts and then IBIs. These are plotted for both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = prepare_file(\"/Users/paul/Library/Mobile Documents/com~apple~CloudDocs/para/2_Projects/modular_cultures/_repo/_latest/dat.nosync/simulations/lif/raw/stim=02_k=3_kin=30_jA=45.0_jG=50.0_jM=15.0_tD=20.0_rate=80.0_stimrate=0.0_rep=001.hdf5\")\n",
    "\n",
    "h5f_new = h5f.clone()\n",
    "\n",
    "\n",
    "# do the filtering\n",
    "spiketrain = h5f[\"data\"][\"spiketimes\"]\n",
    "#Bin the raster plot\n",
    "binned, which_bin = bin_spikes(spiketrain)\n",
    "#Apply the filtering and get its spike count\n",
    "masked = spikes_not_detected(binned, prob_values, prob_table=\"lr_emxs\")\n",
    "#Use this to delete spikes \n",
    "new_spikes = delete_from_spikeseries(binned, masked, which_bin, spiketrain)\n",
    "#Apply the new spiketrain to the copy of the H5F file\n",
    "h5f_new[\"data\"][\"spiketimes\"] = new_spikes\n",
    "\n",
    "# lets have a look:\n",
    "h5f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert results into DataFrame, as expected by the plot violins function\n",
    "\n",
    "#Repeat for a new DF for bursts. \n",
    "df_sl = pd.DataFrame()\n",
    "#Compute the sequence length to see number of modules participating in a burst\n",
    "es_old = [len(seq) for seq in h5f[\"ana.bursts.system_level.module_sequences\"]]\n",
    "es_new = [len(seq) for seq in h5f_new[\"ana.bursts.system_level.module_sequences\"]]\n",
    "df_sl[\"Sequence Length\"] = np.concatenate([es_old, es_new])\n",
    "df_sl[\"Filter\"] = [\"Full\" for k in range(len(es_old))] + [\"Filtered\" for k in range(len(es_new))]\n",
    "\n",
    "#Repeat for IBIs\n",
    "df_ibis = pd.DataFrame()\n",
    "ibis_old = h5f[\"ana.ibi.system_level.any_module\"]\n",
    "ibis_new = h5f_new[\"ana.ibi.system_level.any_module\"]\n",
    "df_ibis[\"IBIs\"] = np.concatenate([ibis_old, ibis_new])\n",
    "df_ibis[\"Filter\"] = [\"Full\" for k in range(len(ibis_old))] + [\"Filtered\" for k in range(len(ibis_new))]\n",
    "\n",
    "#Repeat for a new DF of event sizes\n",
    "df_es = pd.DataFrame()\n",
    "es_old = h5f[\"ana.bursts.system_level.participating_fraction\"]\n",
    "es_new = h5f_new[\"ana.bursts.system_level.participating_fraction\"]\n",
    "df_es[\"Event size\"] = np.concatenate([es_old, es_new])\n",
    "df_es[\"Filter\"] = [\"Full\" for k in range(len(es_old))] + [\"Filtered\" for k in range(len(es_new))]\n",
    "\n",
    "# Correlation\n",
    "rij = ah.find_rij(h5f, time_bin_size=500/1000)\n",
    "np.fill_diagonal(rij, np.nan)\n",
    "rij = rij.flatten()\n",
    "\n",
    "rij_new = ah.find_rij(h5f_new, time_bin_size=500/1000)\n",
    "np.fill_diagonal(rij_new, np.nan)\n",
    "rij_new = rij_new.flatten()\n",
    "\n",
    "df_rij = pd.DataFrame()\n",
    "df_rij[\"Correlation\"] = np.concatenate([rij, rij_new])\n",
    "df_rij[\"Filter\"] = [\"Full\" for k in range(len(rij))] + [\"Filtered\" for k in range(len(rij_new))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all together!!\n",
    "\n",
    "# Paper\n",
    "fig, axes = plt.subplot_mosaic(\n",
    "    \"\"\"\n",
    "AAAA\n",
    "BBBB\n",
    "CDEF\n",
    "\"\"\",\n",
    "    figsize=(6, 3),\n",
    "    gridspec_kw=dict(height_ratios=[0.75, 0.75, 1]),\n",
    ")\n",
    "\n",
    "\n",
    "ph.plot_raster(h5f, axes[\"A\"], zorder=-3)\n",
    "axes[\"A\"].set_xlabel(\"\")\n",
    "ph.plot_raster(h5f_new, axes[\"B\"], zorder=-3)\n",
    "\n",
    "for lab in \"AB\":\n",
    "    axes[lab].set_xlim(100, 406)\n",
    "\n",
    "palette = dict(Full=\"black\", Filtered=\"gray\")\n",
    "\n",
    "ppl.custom_violins(\n",
    "    df_sl, category=\"Filter\", observable=\"Sequence Length\", ax=axes[\"C\"], palette=palette\n",
    ")\n",
    "ppl.custom_violins(\n",
    "    df_es, category=\"Filter\", observable=\"Event size\", ax=axes[\"D\"], palette=palette\n",
    ")\n",
    "ppl.custom_violins(\n",
    "    df_ibis, category=\"Filter\", observable=\"IBIs\", ax=axes[\"E\"], palette=palette\n",
    ")\n",
    "ppl.custom_violins(\n",
    "    df_rij, category=\"Filter\", observable=\"Correlation\", ax=axes[\"F\"], palette=palette\n",
    ")\n",
    "\n",
    "axes[\"C\"].set_ylabel(\"Nubmer of modules\\nparticipating in events\")\n",
    "axes[\"C\"].set_ylim(1, 4)\n",
    "axes[\"D\"].set_ylabel(\"Event size\")\n",
    "axes[\"D\"].set_ylim(0, 1)\n",
    "axes[\"E\"].set_ylabel(\"Inter-event interval\\n(seconds)\")\n",
    "axes[\"E\"].set_ylim(0, None)\n",
    "axes[\"F\"].set_ylabel(\"Neuron correlation\")\n",
    "axes[\"F\"].set_ylim(0, 1)\n",
    "\n",
    "for lab in \"CDEF\":\n",
    "    axes[lab].set_xlabel(\"\")\n",
    "    ppl._noclip(axes[lab])\n",
    "    ppl.sns.despine(ax=axes[lab], bottom=False, offset = 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"effect_filter.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "965c20fcf1f3c4c2d622bc2f64d0068a6184ac03d438688c29b2bf2de8d7c0a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
