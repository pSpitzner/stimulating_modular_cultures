{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model of Spiking Neurons\n",
    "## Getting Started\n",
    "\n",
    "The code to generate spiking data from simulations of leaky integrate and fire neurons is can be found in `src/quadratic_integrate_and_fire.py`.\n",
    "It uses the Brian2 simulator and creates a hdf5 file containing spike times (and meta data) for every set of parameters.\n",
    "\n",
    "For example,\n",
    "```\n",
    ">>> python ./src/quadratic_integrate_and_fire.py -o ./dat/simulations/lif/test.hdf5 -d 300 -equil 60 -r 80\n",
    "```\n",
    "will run a simulation that lasts 300 seconds.\n",
    "\n",
    "Here is a list of the model parameters, that can be specified at run time:\n",
    "```\n",
    ">>> python ./src/quadratic_integrate_and_fire.py -h \n",
    "\n",
    "  -h, --help            show this help message and exit\n",
    "  -o FILE               output path\n",
    "  -jA 45.0              AMPA current strength, in mV\n",
    "  -jG 50.0              GABA current strength, in mV\n",
    "  -jM 15.0              Minis (noise amplitude), in mV\n",
    "  -r 80.0               Poisson rate (minis), all neurons, in Hz\n",
    "  -tD 20.0              Characteristic recovery time, in seconds\n",
    "  -s 42                 RNG seed\n",
    "  -k 5                  Number of bridging axons\n",
    "  -d 1200               Recording duration, in seconds\n",
    "  -equil 120, --equilibrate 120\n",
    "                        Equilibration duration, in seconds\n",
    "  -stim off             if/how to stimulate: 'off', 'poisson'\n",
    "  -stim_rate 20         additional rate in stimulated mods, in Hz\n",
    "  -mod 0                modules to stimulate, e.g. `0`, or `02` for multiple\n",
    "  --bridge_weight 1.0   synaptic weight of bridge neurons [0, 1]\n",
    "  --inhibition_fraction 0.2\n",
    "                        fraction of neurons that should be inhibitory\n",
    "  --record-state-dt 25.0\n",
    "                        time step for recording state variables, in ms.\n",
    "                        use 0.5 to get smooth resource-rate cycles\n",
    "```\n",
    "\n",
    "After creating running a simulation, we can use the helper functions in `ana/plot_helper.py` to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autoreload extension allows you to tweak the code in the imported modules\n",
    "# and rerun cells to reflect the changes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# this allows us to output the dictionary structure\n",
    "%load_ext ipy_dict_hierarchy\n",
    "# matplotlib settings to look decent in the notebook\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# the helpers are in the `/ana/` folder.\n",
    "import sys\n",
    "sys.path.append(\"./../\")\n",
    "from ana import ana_helper as ah\n",
    "from ana import plot_helper as ph\n",
    "from ana import paper_plots as pp\n",
    "\n",
    "# setup logging for notebook\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(name)-12s | %(message)s\",\n",
    "    datefmt=\"%y-%m-%d %H:%M\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "log = logging.getLogger(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load and take a look at the file we have created\n",
    "h5f = ah.prepare_file(\"./../dat/simulations/lif/test.hdf5\")\n",
    "h5f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `h5f` above is just a nested dictionary that corresponds to a loaded hdf5 file. We use those a lot throught the analysis and plotting routines. Whenever an analysis is run, it can use details from known places and add the results directly to the `/ana/` dictionary (without writing to disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silence the detailed outputs\n",
    "ph.log.setLevel(\"ERROR\")\n",
    "# and produce overview panels for the loaded file\n",
    "ph.overview_dynamic(h5f);\n",
    "ph.overview_topology(h5f);\n",
    "# Note, when using an interactive matplotlib backend, you can zoom and pan the figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data\n",
    "We ran two sets of simulations: stimulating the whole system, or only two modules.\n",
    "For example, the `python ./run/create_pars_for_global_stimulation.py` produces `run/parameters.tsv` with the parameter combinations we used, each line can be called from terminal.\n",
    "By default, each line corresponds to one realization and produces one hdf5 file (like above) in `dat/simulations/lif/raw`.\n",
    "\n",
    "Now you would call each line in a bash for-loop, or rather, have a look at `run/submit_to_cluster.sh`\n",
    "For our example of 2k realizations and an SGE: `qsub -t 1-2000 ./run/submit_to_cluster.sh`\n",
    "\n",
    "Note on compute time:\n",
    "We used 50 realizations per coordinate and topology. Each of those (2k in total) runs approximately in real-time, and we simulate 30 minute recordings.\n",
    "\n",
    "### Data for rate-resource cycles\n",
    "To get the rate-resource cylces smooth, we have to record the resource variable during the simulations at high time resolution (which needs a lot of disk space, ~4GB for 30 min).\n",
    "This can be enabled by passing `--record-state-dt 0.5` to `src/quadratic_integrate_and_fire.py`.\n",
    "If you just want to reproduce the figures from the manuscript, you can quickly run a few simulations:\n",
    "```\n",
    "python ./src/quadratic_integrate_and_fire.py -o ./dat/simulations/lif/raw/highres_stim=off_k=-1_jA=45.0_jG=50.0_jM=15.0_tD=20.0_rate=80.0_rep=001.hdf5 -k -1 -d 1800 -equil 300 -s 6102 --record-state-dt 0.5 -r 80\n",
    "python ./src/quadratic_integrate_and_fire.py -o ./dat/simulations/lif/raw/highres_stim=off_k=-1_jA=45.0_jG=50.0_jM=15.0_tD=20.0_rate=90.0_rep=001.hdf5 -k -1 -d 1800 -equil 300 -s 6102 --record-state-dt 0.5 -r 90\n",
    "python ./src/quadratic_integrate_and_fire.py -o ./dat/simulations/lif/raw/highres_stim=off_k=1_jA=45.0_jG=50.0_jM=15.0_tD=20.0_rate=80.0_rep=001.hdf5 -k 1 -d 1800 -equil 300 -s 6002 --record-state-dt 0.5 -r 80\n",
    "python ./src/quadratic_integrate_and_fire.py -o ./dat/simulations/lif/raw/highres_stim=off_k=1_jA=45.0_jG=50.0_jM=15.0_tD=20.0_rate=90.0_rep=001.hdf5 -k 1 -d 1800 -equil 300 -s 6002 --record-state-dt 0.5 -r 90\n",
    "python ./src/quadratic_integrate_and_fire.py -o ./dat/simulations/lif/raw/highres_stim=off_k=5_jA=45.0_jG=50.0_jM=15.0_tD=20.0_rate=80.0_rep=001.hdf5 -k 5 -d 1800 -equil 300 -s 6102 --record-state-dt 0.5 -r 80\n",
    "python ./src/quadratic_integrate_and_fire.py -o ./dat/simulations/lif/raw/highres_stim=off_k=5_jA=45.0_jG=50.0_jM=15.0_tD=20.0_rate=90.0_rep=001.hdf5 -k 5 -d 1800 -equil 300 -s 6102 --record-state-dt 0.5 -r 90\n",
    "python ./src/quadratic_integrate_and_fire.py -o ./dat/simulations/lif/raw/highres_stim=off_k=10_jA=45.0_jG=50.0_jM=15.0_tD=20.0_rate=80.0_rep=001.hdf5 -k 10 -d 1800 -equil 300 -s 6202 --record-state-dt 0.5 -r 80\n",
    "python ./src/quadratic_integrate_and_fire.py -o ./dat/simulations/lif/raw/highres_stim=off_k=10_jA=45.0_jG=50.0_jM=15.0_tD=20.0_rate=90.0_rep=001.hdf5 -k 10 -d 1800 -equil 300 -s 6202 --record-state-dt 0.5 -r 90\n",
    "```\n",
    "\n",
    "## Analysing\n",
    "### Partial stimulation\n",
    "The simulations with two targeted modules are analysed by the same script as the experiments\n",
    "`ana/process_conditions.py`, it produces a pre-stim comparison.\n",
    "\n",
    "The script has the (input and output) filenames hardcoded. It globs the input files that match the right file pattern, depending on `-t sim_partial`, `-t sim_partial_no_inhib` or `-t sim` (global stimulation) and creates and output file matching the `-t` argument.\n",
    "For instance,\n",
    "```\n",
    "python ./ana/process_conditions.py -t sim_partial -i ./dat/simulations/lif/raw/ -o ./dat/simulations/lif/processed/\n",
    "```\n",
    "Scans all files in `/dat/simulations/lif/raw/` and outputs `./dat/simulations/lif/processed/k=5_partial.hdf5`.\n",
    "\n",
    "### Global stimulation\n",
    "For the global stimulation, we mainly varied the input rate (`-r`) which ended up on the x-axis of most plots.\n",
    "\n",
    "In order to combine the many runs in the `dat/simulations/lif/raw/` folder, we use the script in `ana/ndim_merge.py`. It takes a wildcarded file path, runs a lot of analysis, and merges the results into a high-dimensional (x-array like) hdf5 file. As this can take a while, it uses dask and can delegate to a cluster.\n",
    "```\n",
    "python ./ana/ndim_merge.py -i './dat/simulations/lif/raw/stim=off*jM=15*tD=20*.hdf5' -o ./dat/simulations/lif/processed/ndim.hdf5 -c num_cores\n",
    "```\n",
    "\n",
    "Both of the above scripts are wrappers (handling mostly the IO) around the lower-level analysis routines that can be found in `ana/ana_helper.py`. To reproduce where each observable in the processed dataframes comes from, I recommend to skim the scripts above and then jump to the corresponding part in the `ana_helper`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots from the paper\n",
    "### Partial Stimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src import topology\n",
    "\n",
    "topology.log.setLevel(\"ERROR\")\n",
    "log.setLevel(\"INFO\")\n",
    "\n",
    "def _ev(x, pdist):\n",
    "    \"\"\"\n",
    "    expectation value via x, p(x)\n",
    "    \"\"\"\n",
    "    ev = 0\n",
    "    for idx, p in enumerate(pdist):\n",
    "        ev += x[idx] * pdist[idx]\n",
    "    return ev\n",
    "\n",
    "\n",
    "# we want tp find par_alpha so that the expectation value of the in-degree is ~ 30\n",
    "# iterate until we find a value thats close enough by threshold\n",
    "def find_by_iteration(generator, target, init=0.0125, iterstep=0.0025, threshold=0.1):\n",
    "    alpha = init\n",
    "    prev_was_greater = None\n",
    "    while True:\n",
    "        series = generator(alpha)\n",
    "        # calculate the probability distribution from the series\n",
    "        pdist = np.bincount(series, minlength=100) / len(series)\n",
    "        # calculate the expectation value\n",
    "        ev = _ev(np.arange(len(pdist)), pdist)\n",
    "\n",
    "        log.debug(f\"{alpha=:.4f}, {ev=:.3f}\")\n",
    "\n",
    "        if np.abs(ev - target) < threshold:\n",
    "            break\n",
    "        else:\n",
    "            if prev_was_greater is None:\n",
    "                prev_was_greater = ev > target\n",
    "            else:\n",
    "                if prev_was_greater != (ev > target):\n",
    "                    iterstep /= 2\n",
    "                    prev_was_greater = ev > target\n",
    "                    log.debug(f\"new {iterstep=:.3e}\")\n",
    "            # increase or decrease alpha if above or below\n",
    "            if ev > target:\n",
    "                alpha -= iterstep\n",
    "            else:\n",
    "                alpha += iterstep\n",
    "    log.info(f\"final {alpha=:.4f}, {ev=:.3f}\")\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def generate_series_of_k_from_topo(alpha, num_connections, num_reps=10):\n",
    "    \"\"\"\n",
    "    generate a series of in-degrees from a topology specified via num_connections.\n",
    "    -1 for merged\n",
    "    \"\"\"\n",
    "    series = []\n",
    "    for _ in range(num_reps):\n",
    "        if num_connections == -1:\n",
    "            tp = topology.MergedTopology(par_alpha=alpha)\n",
    "        else:\n",
    "            tp = topology.ModularTopology(par_k_inter=num_connections, par_alpha=alpha)\n",
    "        series.extend(tp.k_in)\n",
    "    return np.array(series)\n",
    "\n",
    "\n",
    "alphas3 = dict()\n",
    "for k in [-1, 1, 3, 5, 10, 20]:\n",
    "    log.info(f\"finding alpha for {k=}\")\n",
    "    alpha = find_by_iteration(\n",
    "        generator=lambda alpha: generate_series_of_k_from_topo(alpha, k),\n",
    "        target=30,\n",
    "        threshold=0.1,\n",
    "    )\n",
    "    alphas3[k] = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, alpha in alphas.items():\n",
    "    log.info(f\"{k=}, {round(alpha, 5)}\")\n",
    "\n",
    "for k, alpha in alphas2.items():\n",
    "    log.info(f\"{k=}, {round(alpha, 5)}\")\n",
    "\n",
    "for k, alpha in alphas3.items():\n",
    "    log.info(f\"{k=}, {round(alpha, 5)}\")\n",
    "\n",
    "log.info(\"---\")\n",
    "for k, alpha in alphas3.items():\n",
    "    log.info(f\"{k=}, {round(np.mean([alphas[k], alphas2[k], alphas3[k]]), 5)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.log.setLevel(\"ERROR\")\n",
    "print(pp.fig_3.__doc__)\n",
    "\n",
    "for k in [\"1\", \"5\", \"10\", \"20\", \"-1\"]:\n",
    "    pp.fig_3(\n",
    "        pd_path = f\"{pp.p_sim}/lif/processed/k={k}_partial.hdf5\",\n",
    "        out_suffix=f\"k={k}_partial\"\n",
    "        # raw_paths = [\n",
    "        #         f\"{pp.p_sim}/lif/raw/stim=02_k=5_jA=45.0_jG=50.0_jM=15.0_tD=20.0_rate=80.0_stimrate=0.0_rep=000.hdf5\",\n",
    "        #         f\"{pp.p_sim}/lif/raw/stim=02_k=5_jA=45.0_jG=50.0_jM=15.0_tD=20.0_rate=80.0_stimrate=20.0_rep=000.hdf5\",\n",
    "        #     ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reproduce the figure 3 for the case with blocked inhibition,\n",
    "# we need to tweak the file paths a bit:\n",
    "pp.fig_3(\n",
    "    # processed data frame\n",
    "    pd_path=f\"{pp.p_sim}/lif/processed/k=5_partial_no_inhib.hdf5\",\n",
    "    # raw data for rasters, note jG=0\n",
    "    raw_paths=[\n",
    "        f\"{pp.p_sim}/lif/raw/stim=02_k=5_jA=45.0_jG=0.0_jM=15.0_tD=20.0_rate=80.0_stimrate=0.0_rep=001.hdf5\",\n",
    "        f\"{pp.p_sim}/lif/raw/stim=02_k=5_jA=45.0_jG=0.0_jM=15.0_tD=20.0_rate=80.0_stimrate=20.0_rep=001.hdf5\",\n",
    "    ],\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.fig_3(\n",
    "    # processed data frame\n",
    "    pd_path=f\"{pp.p_sim}/lif/processed/k=-1_partial.hdf5\",\n",
    "    # raw data for rasters, note jG=0\n",
    "    raw_paths=[\n",
    "        f\"{pp.p_sim}/lif/raw/stim=02_k=5_jA=45.0_jG=0.0_jM=15.0_tD=20.0_rate=80.0_stimrate=0.0_rep=001.hdf5\",\n",
    "        f\"{pp.p_sim}/lif/raw/stim=02_k=5_jA=45.0_jG=0.0_jM=15.0_tD=20.0_rate=80.0_stimrate=20.0_rep=001.hdf5\",\n",
    "    ],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimulation in all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pp.fig_4.__doc__)\n",
    "pp.show_legend = False\n",
    "pp.fig_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pp.fig_4_snapshots.__doc__)\n",
    "pp.fig_4_snapshots(\n",
    "    skip_rasters=False,\n",
    "    # plotting resource cycles takes very long.\n",
    "    skip_cycles=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inhomogeneous degree distribtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in [1, 5, 10, -1]:\n",
    "#     pp.sim_degrees_sampled(k_inter=k, num_reps=10)\n",
    "\n",
    "for k in [1]:\n",
    "    ax = pp.sim_degrees_sampled(k_inter=k, num_reps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.legend()\n",
    "ax.set_xlim(0,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('miniforge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a88e3ecbd7c5bf2e5c571339b8074bc204a16e183107704a2f69334594804609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
